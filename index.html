<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Wells-fargo-analytics-competition by squigley17</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Wells-fargo-analytics-competition</h1>
      <h2 class="project-tagline">College of Charleston Data 101 class competition</h2>
      <a href="https://github.com/squigley17/Wells-Fargo-Analytics-Competition" class="btn">View on GitHub</a>
      <a href="https://github.com/squigley17/Wells-Fargo-Analytics-Competition/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/squigley17/Wells-Fargo-Analytics-Competition/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="financial-topics-discussed-on-social-media-and-cause-of-topics" class="anchor" href="#financial-topics-discussed-on-social-media-and-cause-of-topics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Financial Topics Discussed On Social Media and Cause of Topics</h2>

<p>Initially, the social media dataset was cleaned by removing punctuation, uppercase, stopwords, non ascii values, extra space, and high frequency words with little meaning for this research. </p>

<p><code>Words removed:</code> 
<code>name, the, internet, are, excel, were, is, twit, twit_hndl, Name_Resp, name_resp, twit_hndl$, w/, with, i, I, INTERNET, the, they, this, them</code></p>

<p>See the code flow to better understand our overall process to cleaning the data 
<img src="http://i.imgur.com/4PWDtVB.png" alt="code flow"></p>

<p>We created a dendrogram of frequent terms which can also be found in the attached files. We found trends in word relationships like thank-information-share-appreciate-feedbankb-center-financial. We found, from this association, that bank b is doing well with helping customers over social media with provided information about financial centers. </p>

<p>From the word associations and trends, we looked at correlation between frequent terms and other relevant words in the data set. The most commonly used words which most frequently concerned the topics of banking services and words surrounding communications were words such as the words <strong>“please”, “help”, and “call”</strong>. These sounds like words most frequently used to describer customer service calls.</p>

<p><img src="http://i.imgur.com/jGlR0JZ.png" alt="correlation"></p>

<p>The dendrogram indicates the relationship of words to each other using Euclidian distance formula. Words that are more frequently used together appear in branches nearer to each other. </p>

<p>The list of the most frequent words in the dataset included:
<code>able</code>
<code>about</code>
<code>account</code>
<code>after</code>
<code>all</code>
<code>any</code>
<code>anything</code>
<code>apply</code>
<code>appreciate</code>
<code>are</code>
<code>assist</code>
<code>assistance</code>
<code>banking</code>
<code>banks</code>
<code>been</code>
<code>best</code>
<code>business</code>
<code>but</code>
<code>buy</code>
<code>call</code>
<code>called</code>
<code>can</code>
<code>cant</code>
<code>card</code>
<code>center</code>
<code>check</code>
<code>contact</code>
<code>credit</code>
<code>customer</code>
<code>details</code>
<code>did</code>
<code>dirmsg</code>
<code>discuss</code>
<code>dlvr</code>
<code>dont</code>
<code>feedbankb</code>
<code>financial</code>
<code>follow</code>
<code>following</code>
<code>from</code>
<code>full</code>
<code>get</code>
<code>give</code>
<code>going</code>
<code>good</code>
<code>got</code>
<code>grants</code>
<code>great</code>
<code>had</code>
<code>happy</code>
<code>has</code>
<code>have</code>
<code>help</code>
<code>here</code>
<code>how</code>
<code>info</code>
<code>information</code>
<code>into</code>
<code>its</code>
<code>just</code>
<code>know</code>
<code>learn</code>
<code>let</code>
<code>like</code>
<code>look</code>
<code>main</code>
<code>make</code>
<code>may</code>
<code>mission</code>
<code>money</code>
<code>more</code>
<code>need</code>
<code>new</code>
<code>not</code>
<code>now</code>
<code>number</code>
<code>numbers</code>
<code>one</code>
<code>only</code>
<code>our</code>
<code>out</code>
<code>over</code>
<code>pay</code>
<code>phone</code>
<code>photo</code>
<code>please</code>
<code>program</code>
<code>rating</code>
<code>see</code>
<code>service</code>
<code>share</code>
<code>shared</code>
<code>small</code>
<code>some</code>
<code>sorry</code>
<code>still</code>
<code>street</code>
<code>swing</code>
<code>take</code>
<code>tell</code>
<code>thank</code>
<code>thanks</code>
<code>that</code>
<code>their</code>
<code>there</code>
<code>they</code>
<code>this</code>
<code>time</code>
<code>today</code>
<code>try</code>
<code>via</code>
<code>visit</code>
<code>vote</code>
<code>want</code>
<code>well</code>
<code>were</code>
<code>what</code>
<code>when</code>
<code>who</code>
<code>why</code>
<code>will</code>
<code>with</code>
<code>would</code></p>

<h3>
<a id="dendrogram" class="anchor" href="#dendrogram" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dendrogram</h3>

<p><img src="http://i.imgur.com/0DxEof5.jpg" alt="dendrogram"></p>

<p>In addition to the list of frequent terms from our cleaned data, we ran a series of frequent term analyses based on parts of the dataset containing certain words.</p>

<p>Here is the code used with the example term being "Service"</p>

<p>service.idx = df.idx[sapply(df.idx,function(x) grepl("service",x))]
service.corp &lt;- VCorpus(VectorSource(service.idx))
service.dtm &lt;- DocumentTermMatrix(service.corp)
service.freqterms = findFreqTerms(service.dtm, 100)</p>

<p>Here are the important words found from searching:</p>

<p><img src="http://i.imgur.com/dI1BTTy.png" alt=""></p>

<p>We performed a sentitiment analysis on the subsets. They were then compiled and we created a boxplot of the distribution of the scores between topics. (I was not able to create the visual though, because R stopped working)</p>

<p>int.scores = score.sentiment(int.idx, pos, neg, .progress='text')
service.scores = score.sentiment(service.idx, pos, neg, .progress='text')
help.scores = score.sentiment(help.idx, pos, neg, .progress='text')
loan.scores = score.sentiment(loan.idx, pos, neg, .progress='text')</p>

<p>boxplot(loan.scores$score,help.scores$score,service.scores$scores,int.scores) </p>

<p>Overall, we found that a majority of the conversations were pertaining to customers needing help and going onto social media to seek it. From the trends, we can assume most of these issues were successfully resolved, though they were taken off of public social media to do so.</p>

<h2>
<a id="are-the-topics-and-substance-consistent-across-the-industry-or-are-they-isolated-to-individual-banks" class="anchor" href="#are-the-topics-and-substance-consistent-across-the-industry-or-are-they-isolated-to-individual-banks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Are the topics and substance consistent across the industry or are they isolated to individual banks?</h2>

<p>We used the corpus to create datasets that were associated with each bank. </p>

<p><img src="http://i.imgur.com/oNzxbjf.png" alt="bank"></p>

<p>Each dataset was then tested against an existing list of positive and negative words to find a sentiment score of each word in the dataset. In order to quantify the disparity between the mean sentiments of the banks as individuals compared to that of the entire data set, a T-test was performed. </p>

<p>The T-tests below check to see if there is a difference in the mean sentiment between the entire data set and social media posts mentioning specific banks (BankA, BankB, BankC, and BankD).</p>

<pre><code>Bank A Sentiment Scores
</code></pre>

<blockquote>
<p>t.test(scores$score,scores.A$score,var.equal=T)</p>
</blockquote>

<p>t = -14.333, df = 277960, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval: -0.09243726 -0.07019727
sample estimates: mean of x mean of y 0.1390526 0.2203699</p>

<pre><code>Bank B Sentiment Scores
</code></pre>

<blockquote>
<p>t.test(scores$score,scores.B$score,var.equal=T)</p>
</blockquote>

<p>data:  scores$score and scores.B$score
t = 11.626, df = 271550, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval: 0.05733344 0.08058343
sample estimates: mean of x  mean of y 0.13905262 0.07009419 </p>

<pre><code>Bank C Sentiment Scores
</code></pre>

<blockquote>
<p>t.test(scores$score,scores.C$score,var.equal=T)</p>
</blockquote>

<p>data:  scores$score and scores.C$score
t = 12.667, df = 247920, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval: 0.08046068 0.10991763
sample estimates: mean of x  mean of y 0.13905262 0.04386347 </p>

<pre><code>Bank D Sentiment Scores
</code></pre>

<blockquote>
<p>t.test(scores$score,scores.D$score,var.equal=T)</p>
</blockquote>

<p>data:  scores$score and scores.D$score
t = 19.438, df = 273000, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:0.1003944 0.1229112
sample estimates: mean of x  mean of y 0.13905262 0.02739986 </p>

<p>The null hypothesis was that there would be no difference between the groups. In every case, the p-value was overwhelmingly small (2.2 x 10-16), so the null hypothesis can be rejected with a very high degree of certainty. On average, the BankA posts scored higher on the sentiment analysis than the general posts. The mean sentiment of each of the remaining 3 banks was lower than the general average. BankD scored the worst, followed by BankC, and then BankB. Looking at the violin plots, the normality assumption of the t-tests seems to be fulfilled as well as the assumption of equal variance. 
<img src="http://i.imgur.com/39y1fkT.png" alt="violin plot"> 
Some of the groups have shortened tails, but because of how large the dataset was, this is not very worrisome. The outliers also do not present a source of issue as the large size of the dataset keeps them from being particularly influential and unusual values are bound to pop up in a dataset composed of thousands of pieces of data. Thus, it can be concluded from the sentiment analysis that which bank the customer is talking about does in fact have an effect on how positive or negative the social media post is (for this dataset).</p>

<p>The null hypothesis was that there would be no difference between the mean sentiments of a given bank compared to that of mean sentiment of the banks combined. Please see the attached document titled Statistical analysis to see each test. Looking at the violin plots, the normality assumption of the t-tests appears to be fulfilled as well as the assumption of equal variance. Bank c and d have shortened tails, however, due to the size of the data set, it can be determined that the t-test qualifies as an appropriate model. The outliers are of no concern due to the number of observations in the dataset and their affect is therefore negligible. 
Factors considered, it is safe to conclude a relationship exists between the bank being discussed in a tweet and the positivity or negativity of the language in the tweet. In every case, the p-value was overwhelmingly small (2.2 x 10-16). The null hypothesis can be rejected with 100% confidence. On average, the BankA posts scored higher on the sentiment analysis than the general posts. The mean sentiment of each of the remaining 3 banks was lower than the general average. BankD scored the worst, followed by BankC, and then BankB. </p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/squigley17/Wells-Fargo-Analytics-Competition">Wells-fargo-analytics-competition</a> is maintained by <a href="https://github.com/squigley17">squigley17</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
