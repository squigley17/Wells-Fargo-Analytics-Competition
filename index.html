<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Wells-fargo-analytics-competition by squigley17</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Wells-fargo-analytics-competition</h1>
      <h2 class="project-tagline">College of Charleston Data 101 class competition</h2>
      <a href="https://github.com/squigley17/Wells-Fargo-Analytics-Competition" class="btn">View on GitHub</a>
      <a href="https://github.com/squigley17/Wells-Fargo-Analytics-Competition/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/squigley17/Wells-Fargo-Analytics-Competition/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="financial-topics-discussed-on-social-media-and-cause-of-topics" class="anchor" href="#financial-topics-discussed-on-social-media-and-cause-of-topics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Financial Topics Discussed On Social Media and Cause of Topics</h2>

<p>Initially, the social media dataset was cleaned by removing punctuation, uppercase, stopwords, non ascii values, extra space, and high frequency words with little meaning for this research. </p>

<p><code>Words removed:</code> 
<code>name, the, internet, are, excel, were, is, twit, twit_hndl, Name_Resp, name_resp, twit_hndl$, w/, with, i, I, INTERNET, the, they, this, them</code></p>

<p>Please see the code flow to better understand our overall process to cleaning the data. 
<img src="http://i.imgur.com/mlfVpv4.png" alt="code flow"></p>

<p>We created a dendrogram of frequent terms which can also be found in the attached files. From these, we looked at correlation between frequent terms and other relevant words in the data set. Some interesting correlations can be found in the document titled “correlations.” The most commonly used words which most frequently concerned the topics of banking services and words surrounding communications were words such as the words <strong>“please”, “help”, and “call”</strong>. </p>

<blockquote>
<p>findAssocs(dtm.clean, "call", 0.1)
  phone discuss    give  number  please   happy    best   visit 
   0.43    0.27    0.22    0.21    0.19    0.15    0.11    0.11 </p>

<p>findAssocs(dtm.clean, "business", 0.1)
  small mission   swing    main    vote  street program   today   apply  grants   learn    full    here    more 
   0.59    0.58    0.58    0.57    0.57    0.56    0.55    0.55    0.53    0.52    0.50    0.46    0.35    0.31 </p>

<p>findAssocs(dtm.clean, "best", 0.1)
   time  dirmsg   phone  please    call contact    well 
   0.28    0.18    0.15    0.13    0.11    0.11    0.10 </p>

<p>findAssocs(dtm.clean, "account", 0.1)
numbers    tell     try     any   about    with  dirmsg  follow    help    like     not 
   0.32    0.18    0.17    0.13    0.12    0.12    0.11    0.11    0.10    0.10    0.10 </p>

<p>findAssocs(dtm.clean, "please", 0.1)
     dirmsg         let       phone  assistance        help         can        need      follow         see 
       0.47        0.29        0.27        0.26        0.24        0.23        0.23        0.22        0.21 
       know        call   following    anything     discuss        like       happy      assist        best 
       0.20        0.19        0.19        0.17        0.16        0.16        0.15        0.13        0.13 
        any     details     numbers     contact        give        info       tweet information 
       0.12        0.12        0.12        0.11        0.11        0.11        0.11        0.10 </p>

<p>findAssocs(dtm.clean, "help", 0.1)
       see  following        can       need assistance        let     please   anything       know        how 
      0.40       0.36       0.33       0.31       0.30       0.28       0.24       0.22       0.22       0.21 
      like     dirmsg      happy        try      still       able      phone    account       info 
      0.18       0.17       0.17       0.17       0.16       0.15       0.11       0.10       0.10 </p>

<p>findAssocs(dtm.clean, "today", 0.1)
 mission    swing     main     vote   street  program    apply   grants    small    learn     full     here business     more 
    0.94     0.94     0.93     0.92     0.91     0.89     0.87     0.84     0.83     0.82     0.75     0.58     0.55     0.48 </p>

<p>findAssocs(dtm.clean, "small", 0.1)
 mission    swing     main     vote   street  program    today    apply   grants    learn     full business     here     more 
    0.88     0.88     0.87     0.86     0.85     0.83     0.83     0.81     0.78     0.77     0.70     0.59     0.54     0.45 </p>

<p>findAssocs(dtm.clean, "vote", 0.1)
 mission    swing     main  program   street   grants    today    apply    small    learn     full     here business     more 
    0.98     0.98     0.97     0.95     0.94     0.93     0.92     0.90     0.86     0.85     0.78     0.60     0.57     0.50 </p>

<p>findAssocs(dtm.clean, "apply", 0.1)
   swing  mission     main     vote   street  program    today   grants    small    learn     full     here business     more 
    0.92     0.91     0.90     0.90     0.88     0.87     0.87     0.81     0.81     0.80     0.73     0.56     0.53     0.47</p>

<p>findAssocs(dtm.clean, "mission", 0.1)
    main    swing     vote   street  program    today    apply   grants    small    learn     full     here business     more 
    0.99     0.99     0.98     0.96     0.94     0.94     0.91     0.89     0.88     0.86     0.79     0.61     0.58     0.51</p>
</blockquote>

<p>The dendrogram indicates the relationship of words to each other using Euclidian distance formula. Words that are more frequently used together appear in branches nearer to each other. </p>

<p>The list of the most frequent words in the dataset included:
<code>able</code>
<code>about</code>
<code>account</code>
<code>after</code>
<code>all</code>
<code>any</code>
<code>anything</code>
<code>apply</code>
<code>appreciate</code>
<code>are</code>
<code>assist</code>
<code>assistance</code>
<code>banking</code>
<code>banks</code>
<code>been</code>
<code>best</code>
<code>business</code>
<code>but</code>
<code>buy</code>
<code>call</code>
<code>called</code>
<code>can</code>
<code>cant</code>
<code>card</code>
<code>center</code>
<code>check</code>
<code>contact</code>
<code>credit</code>
<code>customer</code>
<code>details</code>
<code>did</code>
<code>dirmsg</code>
<code>discuss</code>
<code>dlvr</code>
<code>dont</code>
<code>feedbankb</code>
<code>financial</code>
<code>follow</code>
<code>following</code>
<code>from</code>
<code>full</code>
<code>get</code>
<code>give</code>
<code>going</code>
<code>good</code>
<code>got</code>
<code>grants</code>
<code>great</code>
<code>had</code>
<code>happy</code>
<code>has</code>
<code>have</code>
<code>help</code>
<code>here</code>
<code>how</code>
<code>info</code>
<code>information</code>
<code>into</code>
<code>its</code>
<code>just</code>
<code>know</code>
<code>learn</code>
<code>let</code>
<code>like</code>
<code>look</code>
<code>main</code>
<code>make</code>
<code>may</code>
<code>mission</code>
<code>money</code>
<code>more</code>
<code>need</code>
<code>new</code>
<code>not</code>
<code>now</code>
<code>number</code>
<code>numbers</code>
<code>one</code>
<code>only</code>
<code>our</code>
<code>out</code>
<code>over</code>
<code>pay</code>
<code>phone</code>
<code>photo</code>
<code>please</code>
<code>program</code>
<code>rating</code>
<code>see</code>
<code>service</code>
<code>share</code>
<code>shared</code>
<code>small</code>
<code>some</code>
<code>sorry</code>
<code>still</code>
<code>street</code>
<code>swing</code>
<code>take</code>
<code>tell</code>
<code>thank</code>
<code>thanks</code>
<code>that</code>
<code>their</code>
<code>there</code>
<code>they</code>
<code>this</code>
<code>time</code>
<code>today</code>
<code>try</code>
<code>via</code>
<code>visit</code>
<code>vote</code>
<code>want</code>
<code>well</code>
<code>were</code>
<code>what</code>
<code>when</code>
<code>who</code>
<code>why</code>
<code>will</code>
<code>with</code>
<code>would</code></p>

<h3>
<a id="dendrogram" class="anchor" href="#dendrogram" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dendrogram</h3>

<p><img src="http://i.imgur.com/0DxEof5.jpg" alt="dendrogram"></p>

<h2>
<a id="are-the-topics-and-substance-consistent-across-the-industry-or-are-they-isolated-to-individual-banks" class="anchor" href="#are-the-topics-and-substance-consistent-across-the-industry-or-are-they-isolated-to-individual-banks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Are the topics and substance consistent across the industry or are they isolated to individual banks?</h2>

<p>We used the corpus to create datasets that were associated with each bank. </p>

<p><img src="http://i.imgur.com/oNzxbjf.png" alt="bank"></p>

<p>Each dataset was then tested against an existing list of positive and negative words to find a sentiment score of each word in the dataset. In order to quantify the disparity between the mean sentiments of the banks as individuals compared to that of the entire data set, a T-test was performed. </p>

<p>The T-tests below check to see if there is a difference in the mean sentiment between the entire data set and social media posts mentioning specific banks (BankA, BankB, BankC, and BankD).</p>

<pre><code>Bank A Sentiment Scores
</code></pre>

<blockquote>
<p>t.test(scores$score,scores.A$score,var.equal=T)</p>
</blockquote>

<p>t = -14.333, df = 277960, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval: -0.09243726 -0.07019727
sample estimates: mean of x mean of y 0.1390526 0.2203699</p>

<pre><code>Bank B Sentiment Scores
</code></pre>

<blockquote>
<p>t.test(scores$score,scores.B$score,var.equal=T)</p>
</blockquote>

<p>data:  scores$score and scores.B$score
t = 11.626, df = 271550, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval: 0.05733344 0.08058343
sample estimates: mean of x  mean of y 0.13905262 0.07009419 </p>

<pre><code>Bank C Sentiment Scores
</code></pre>

<blockquote>
<p>t.test(scores$score,scores.C$score,var.equal=T)</p>
</blockquote>

<p>data:  scores$score and scores.C$score
t = 12.667, df = 247920, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval: 0.08046068 0.10991763
sample estimates: mean of x  mean of y 0.13905262 0.04386347 </p>

<pre><code>Bank D Sentiment Scores
</code></pre>

<blockquote>
<p>t.test(scores$score,scores.D$score,var.equal=T)</p>
</blockquote>

<p>data:  scores$score and scores.D$score
t = 19.438, df = 273000, p-value &lt; 2.2e-16
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:0.1003944 0.1229112
sample estimates: mean of x  mean of y 0.13905262 0.02739986 </p>

<p>The null hypothesis was that there would be no difference between the groups. In every case, the p-value was overwhelmingly small (2.2 x 10-16), so the null hypothesis can be rejected with a very high degree of certainty. On average, the BankA posts scored higher on the sentiment analysis than the general posts. The mean sentiment of each of the remaining 3 banks was lower than the general average. BankD scored the worst, followed by BankC, and then BankB. Looking at the violin plots, the normality assumption of the t-tests seems to be fulfilled as well as the assumption of equal variance. <img src="http://i.imgur.com/39y1fkT.png" alt="violin plot"> Some of the groups have shortened tails, but because of how large the dataset was, this is not very worrisome. The outliers also do not present a source of issue as the large size of the dataset keeps them from being particularly influential and unusual values are bound to pop up in a dataset composed of thousands of pieces of data. Thus, it can be concluded from the sentiment analysis that which bank the customer is talking about does in fact have an effect on how positive or negative the social media post is (for this dataset).</p>

<p>The null hypothesis was that there would be no difference between the mean sentiments of a given bank compared to that of mean sentiment of the banks combined. Please see the attached document titled Statistical analysis to see each test. Looking at the violin plots, the normality assumption of the t-tests appears to be fulfilled as well as the assumption of equal variance. Bank c and d have shortened tails, however, due to the size of the data set, it can be determined that the t-test qualifies as an appropriate model. The outliers are of no concern due to the number of observations in the dataset and their affect is therefore negligible. 
Factors considered, it is safe to conclude a relationship exists between the bank being discussed in a tweet and the positivity or negativity of the language in the tweet. In every case, the p-value was overwhelmingly small (2.2 x 10-16). The null hypothesis can be rejected with 100% confidence. On average, the BankA posts scored higher on the sentiment analysis than the general posts. The mean sentiment of each of the remaining 3 banks was lower than the general average. BankD scored the worst, followed by BankC, and then BankB. </p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/squigley17/Wells-Fargo-Analytics-Competition">Wells-fargo-analytics-competition</a> is maintained by <a href="https://github.com/squigley17">squigley17</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
